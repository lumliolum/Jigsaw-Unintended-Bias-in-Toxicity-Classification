{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import operator\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,roc_auc_score,log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "\n",
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def init_func(worker_id):\n",
    "    np.random.seed(SEED+worker_id)\n",
    "\n",
    "tqdm.pandas()\n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)\n",
    "\n",
    "# noting down the run time of the kernel\n",
    "t1=datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45) (97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "2  59852            ...                                    4\n",
       "3  59855            ...                                    4\n",
       "4  59856            ...                                   47\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>Jeff Sessions is another one of Trump's Orwell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>I actually inspected the infrastructure on Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>No it won't . That's just wishful thinking on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>Instead of wringing our hands and nibbling the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>how many of you commenters have garbage piled ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7000000  Jeff Sessions is another one of Trump's Orwell...\n",
       "1  7000001  I actually inspected the infrastructure on Gra...\n",
       "2  7000002  No it won't . That's just wishful thinking on ...\n",
       "3  7000003  Instead of wringing our hands and nibbling the...\n",
       "4  7000004  how many of you commenters have garbage piled ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "test=pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "sample=pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\")\n",
    "print(train.shape,test.shape)\n",
    "\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "train[identity_columns]=train[identity_columns].fillna(0)\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences,verbose=True):\n",
    "    vocab={}\n",
    "    for sentence in tqdm(sentences,disable=(not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word]+=1\n",
    "            except KeyError:\n",
    "                vocab[word]=1\n",
    "                \n",
    "    print(\"Number of words found in vocab are\",len(vocab.keys()))\n",
    "    return dict(sorted(vocab.items(), key=operator.itemgetter(1))[::-1])\n",
    "\n",
    "def sen(x):\n",
    "    return x.split()\n",
    "\n",
    "def check_coverage(vocab,embeddings_dict):\n",
    "    # words that dont have embeddings\n",
    "    oov={}\n",
    "    # stores words that have embeddings\n",
    "    a=[]\n",
    "    i=0\n",
    "    k=0\n",
    "    for word in tqdm(vocab.keys()):\n",
    "        if embeddings_dict.get(word) is not None:                    # implies that word has embedding\n",
    "            a.append(word)\n",
    "            k=k+vocab[word]\n",
    "        else:\n",
    "            oov[word]=vocab[word]\n",
    "            i=i+vocab[word]\n",
    "    \n",
    "    print(\"Total embeddings found in vocab are\",len(a)/len(vocab)*100,\"%\")\n",
    "    print(\"Total embeddings found in text are\",k/(k+i)*100,\"%\")\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return dict(sorted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:13<00:00, 130697.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:11<00:00, 161442.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [05:05<00:00, 5903.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:03<00:00, 501741.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:11<00:00, 151501.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1804874/1804874 [00:57<00:00, 31353.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:00<00:00, 125889.08it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:00<00:00, 165458.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:16<00:00, 5956.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:00<00:00, 463640.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:00<00:00, 152976.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97320/97320 [00:03<00:00, 31017.25it/s]\n"
     ]
    }
   ],
   "source": [
    "contraction_mapping={\"ain't\": 'is not', \"amn't\": 'am not', \"aren't\": 'are not', \n",
    "                     \"can't\": 'cannot',\"CAN'T\":\"CANNOT\",\"'cause\": 'because', \"could've\": 'could have', \n",
    "                     \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \n",
    "                     \"daresn't\": 'dare not', \"dasn't\": 'dare not', \n",
    "                     \"didn't\": 'did not', \"doesn't\": 'does not', \n",
    "                     \"don't\": 'do not',\"DON'T\":\"DO NOT\",\"e'er\": 'ever', \"everyone's\": 'everyone is', \n",
    "                     'givâ€™n': 'given', \"gon't\": 'go not', \"hadn't\": 'had not', \n",
    "                     \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \n",
    "                     \"he'll\": 'he will', \"he's\": 'he is', \n",
    "                     \"he've\": 'he have', \"how'd\": 'how would', \n",
    "                     'howdy': 'how do you do', \"how'll\": 'how will', \n",
    "                     \"how're\": 'how are', \"how's\": 'how is', \"I'd\": 'I would', \n",
    "                     \"I'll\": 'I will', \"I'm\": 'I am', \"i'm\":\"i am\",\n",
    "                     \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to',\"i've\":\"i have\",\n",
    "                     \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'll\": 'it will', \n",
    "                     \"it's\": 'it is',\"IT'S\":\"IT IS\",\"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \n",
    "                     \"may've\": 'may have', \"mightn't\": 'might not', \n",
    "                     \"might've\": 'might have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \n",
    "                     \"must've\": 'must have', \"needn't\": 'need not', \n",
    "                     \"ne'er\": 'never', \"o'clock\": 'of the clock', \"o'er\": 'over', \"ol'\": 'old', \n",
    "                     \"oughtn't\": 'ought not', \"shalln't\": 'shall not', \"shan't\": 'shall not', \n",
    "                     \"she'd\": 'she would', \n",
    "                     \"she'll\": 'she will', \"she's\": 'she is', \"should've\": 'should have', \n",
    "                     \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \n",
    "                     \"somebody's\": 'somebody is', \n",
    "                     \"someone's\": 'someone is', \"something's\": 'something is', \n",
    "                     \"so're\": 'so are', \"that'll\": 'that will', \"that're\": 'that are', \n",
    "                     \"that's\": 'that is', \"that'd\": 'that had', \n",
    "                     \"there'd\": 'there would', \"there'll\": 'there will',\n",
    "                     \"there're\": 'there are', \"there's\": 'there is',\n",
    "                     \"these're\": 'these are', \"they'd\": 'they would', \n",
    "                     \"they'll\": 'they will', \"they're\": 'they are', \n",
    "                     \"they've\": 'they have', \"this's\": 'this is', \n",
    "                     \"those're\": 'those are', \"'tis\": 'it is', \"'twas\": 'it was', \"wasn't\": 'was not', \n",
    "                     \"we'd\": 'we would', \"we'd've\": 'we would have', \n",
    "                     \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \n",
    "                     \"what'd\": 'what did', \"what'll\": 'what will', \"what're\": 'what are',\n",
    "                     \"what's\": 'what is', \"what've\": 'what have', \n",
    "                     \"when's\": 'when is', \"where'd\": 'where did', \n",
    "                     \"where're\": 'where are', \"where's\": 'where is', \n",
    "                     \"where've\": 'where have', \"which's\": 'which is', \"who'd\": 'who would', \n",
    "                     \"who'd've\": 'who would have', \"who'll\": 'who will', \"whom'st\": 'whom hast', \n",
    "                     \"whom'st'd've\": 'whom hast had have', \"who're\": 'who are', \n",
    "                     \"who's\": 'who is', \"who've\": 'who have', \n",
    "                     \"why'd\": 'why did', \"why're\": 'why are', \"why's\": 'why is', \n",
    "                     \"won't\": 'will not', \"would've\": 'would have', \"wouldn't\": 'would not', \n",
    "                     \"y'all\": 'you all', \"y'all'd've\": 'you all would have', \n",
    "                     \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have',\n",
    "                    \"here's\":\"here is\"}\n",
    "\n",
    "mispelled_dict={'tRump':\"trump\",\"gov't\":\"governement\",\"Gov't\":\"Government\",\"Brexit\":\"British exit\",\n",
    "               \"sayin'\":\"saying\",\"Qur'an\":\"Quran\",\"Drumpf\":\"Donald Trump\",'SB91':\"senate bill\",\n",
    "               \"FAKEnews\":\"FAKE NEWS\",\"OBAMAcare\":\"obamacare\",\"TheDonald\":\"The Donald\"}\n",
    "\n",
    "# adding single puntucation mark...\n",
    "# puncts=[',','.','!','$','(',')','%','[',']','?',':',\";\",\"#\",'/','\"',\n",
    "#                 \"-\",\"|\",'*',\"'\",\"+\",\">\",\"<\",\"=\",\"\\n\",\"^\",\"-\",\"\"]\n",
    "puncts = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + \"'\" + 'â€œâ€â€™' \n",
    "\n",
    "# specials characters are also included in this...\n",
    "puncts_mapping={'â€”':\"-\", \"â€“\":\"-\" ,'â€':'\"', \"â€™\":\"'\", \"Â´\": \"'\", \"`\":\"'\" , 'â€¦':'...', \"\\'\":\"'\",\n",
    "                'â€œ':'\"' ,\"âœ°\":\"*\",\"á´€É´á´…\":\"AND\",\"â€•\":\"-\",\"á´›Êœá´‡\":\"THE\",\"â€¦and\":\"... and\",\n",
    "               \"Ã©\":\"e\",\"á´€\":\"A\",\"Ã¯\":\"i\",\"Ã \":\"a\"}\n",
    "\n",
    "symbols_to_delete='\\n\\xadá´‡á´É´á´µ\\u2004á´›\\tá´€Ê€Êœá´á´œá´„ÊÊŸá´…Ò“á´‹Ê™É¢\\u200bá´¡á´˜ðŸ˜‚ï¸ð’†ðŸ˜ðŸ”¥ðŸŒ®ðŸ˜‰ð’•ð’‚Ñ‚ð’ŠðŸ˜€ð’ðŸ†˜ð’\\u2005á´ŠðŸ‘ðŸ™„ðŸ˜Šá´ Ð½Ð¾\\u200eâ–±\\x7fÎ¹ð’”ð™š\\u2006ð’“ðŸ˜ƒðŸ˜„Ðµ\\u2009\\u2028ð’‰ðŸ˜†ðŸ’€ðŸ‘ŽðŸ˜œð™–ðŸ˜¡ð’…ð’ð™©ð™¤ðŸ¤”ðŸŒ¯Î¿ðŸ˜ŽðŸ’¨Ñ€ÎœðŸ¤£Ñ•ð’–ð™£ð™§ðŸ’¥ð’„Ð¼ð—²Ð»ðŸ’™ð™¨â•ŒÏ‚ðŸ»ð’Ž\\u200a\\u202cðŸ˜¢ð™žÏ…\\u200fðŸ˜…ð’‘Ðºð™¡Ð´ðŸ˜‡\\u202aðŸ’©ðŸ‡ºØ§ð™™ð’šð—¼ð™ð˜ðŸ‡¸ðŸ‘ÑŒÑƒÎºðŸ™ð’‡ð’ˆð™ªÐ¿ð’˜Ù„ð—¶\\x10\\rð’—ðžð—»ðŸ˜³ð—¿ð–”ð™®ð­ð—®Ð¹ðŸ˜ð’ƒð™¥ð™˜Ð±ðŸ™‚ðŸ¼ð˜€ðŸ˜ðŸ‘¥\\u2002ð˜¦\\x08ð–ŠÑðŸ˜•ðŸ˜‘ðŸ˜¬ðŸ˜ð™›ðŸ˜­ð—µðšÑ‹â¤µà®œ\\uf0a7\\u200dð˜µðŸ¾\\ufeffðŸ˜©ð™œÑ‡ð˜ªðŸ˜˜ð—¹Ð·ðŸ˜²ðŸ˜ ðŸ˜ˆð–™ï½…ð˜‚â„Ð³Î¬ð–“ð˜†ðŸ¤¡ðŸ˜’ðŸ’°ðŸ˜”ð–šð’Œð™¬ð—±\\u2003ðŸ˜´ðŸ™ˆðŸ½ðŸ˜±ï¼·Ñ†\\uf0b7ðŸ‘¤ðŸ¤“ð˜¢â€‘ðŸŽ¶\\u3000ï½Žð™—ð¨ð˜°ðŸ˜žÎ¯ÙŠð™¢ð°ð«ãƒ„ðŸ¤‘ðŸ˜£Ù…Ø¨ï½”ð–†ðŸ‘‹ðŸ‡¨ðŸ‘¿ðŸ˜›ð˜´ðŸ˜¨ð–žð–—ð˜¯ð—°ðŸ‡¦ðŸ±ðŸ’”ðŸŒŸð–‰ð–ŽðŸ¦Šð™ƒâ–ðŸ˜°ÏŒðŸ†ÐðŸ˜¥ðŸ˜ŸðŸ˜ð¥ð¬ð¡çš„Ð¡Íž×™Ù†ðŸð—º\\u200cð˜¥ð®ðŸ‡·ðŸŽ¼ÑˆÎ­ðŸ¤—ÐŸ×•Ø±ð‘°ð–‘ðŸ¤·ð‘»ð—½ð—¯Ð¶ðŸ˜¯ðŸŸðŸ•ð˜³Ù‡ð˜­ð—³ÐœðŸ¤§ðŸ’œ\\u202dÑ…ðŸ™Œð˜®ðŸ™ƒðŸ˜ªðŸ˜¤â€¼â©›ðŸ˜¦ð™ ð¦ðœÑŽÐ¯å¤§ï¿¼ðŸ‘Œâ€ðŸ˜–ÙˆØ¹ï½•ï½‰ðŸ‡³ð–œð™«ð›ð§Ð”ðŸ˜µðŸ™Øªá½¶ðŸ˜®×œØ£Ù‚ð˜©ð–‹ð–’ð–ð˜¶æˆ‘Ñ‰Ð æœ‰å–ðŸ¤¢è±†åœŸå°ðŸ‘†ð˜£ðŸ’•ðŸŽ‰á¼€á¼ðŸ˜‹ï½„ð–˜ð˜ƒð˜¼ï½ˆðŸ†ðŸÐ˜ðŸ’šð˜¤ðŸŒºØ©Ø³ðŸ’ªðŸ’¯×ðŸ˜§á¸µð—´ðŸ¤•á¿†ð¯ð¢ð©ðŸ˜·äº†ðŸ”ðŸŒŽç¨ŽåŠ åŽ»å›½ðŸ‡¹ðŸ‘‰ðŸ™‰Ø­Î®×©ðŸµï½’ð’™ðŸ¸ð‘ªð™€ð‘¶ðŸ”«ðŸŽ«ðŸ”á¿¦ðŸ˜«ðŸ‘ðŸ¤¥é’±æ‰€ä½ Ð—ä¸×¨ðŸ™ŠðŸ‘€ð˜§Ø²Ùƒ×¢\\x81ðŸ’–Ø®Ø¯\\uf04aá¿–á½ºðŸ¤žð‘¬á½°ð™”â›·ð™­ðŸ€Ø¬ðï½™ð˜„ðŸ‡ªðŸ¤ ðŸ¤¤ÍŸðŸŽµæ‹¿å¤±æ˜¯äººðŸŒžðŸ¤×”ÙðŸ‘®Î¾á½²×á½´á½‘ï½‚ï½†ð‘·ã€ð‘«ð‘¹ð–•ð–Œð™ˆë‹¤ä»–ä¸­â„´ð“‰ð™„æ”¶ð‘´Ð‘Ð¢ð“çœ‹ðŸ—½ðŸ¤¦ðŸŒÏá½á¼”ä¼šã¤æ–‡\\u202fðŸŒðŸ˜ŒâºðŸ‘„×“×žÊ°áµ—ðŸ¾ð˜ºðŸ˜“\\u2008ðŸš²ðŸ™€ðŸ‘ŠðŸ•á¿¶á¾½ðŸ”¼ðâ™²ð˜“ð˜¸ð˜±Õ¡ð‘ºã€Œð’’ð‘¯ðŸ‰ð‘±ð˜¨ð‘¾ðŸš¿ð–ˆð•´ð–›æ²¡æ³•æƒ³ð’¾â„¯â‚µë‹ˆì–´ðŸ±è¦ðŸˆæ¥ðŸ’ðŸ¤™ç¾ŽðŸ‘¹ðŸ˜™ðŸ‘½ðŸƒð™‡ðŸ’ð…ð˜ð²ðƒï½ŒÊðŸ‡®è¯´å¯×§Ð¤ðŸ’ƒ\\x9dðŸ’‹ðŸŒ¸ðŸ’¤×ªðŸš¬ðŸ’›ðŸ’ŽðŸ’«Ð×—ãƒ³ðŸ’µï¿½â—á¼°ðŸ‡´ðŠâž•â†´ÏŽð˜·ð˜¬Î›ð‘©Ð“ð—•ð‘®ÛŒðŸ‘¨ðŸ´Ñ„\\uf020ðŸ‡µð‘µð™’ð‘²ð’›ð–ë§ˆì—ë‚˜í•©ì•¼í•´ì„ì´ë„ðŸ˜¶ðŸ¦æˆ˜æ•™æœ¬å’ŒðŸ­Î¶ð’½ð“ƒð“‡ð’¶Ê–Íœãƒ‹ðŸ¬ä¹°å·¨ðŸ¦„ðŸš‚Ð£ðŸ·ðŸ‘ºðŸ’˜ðŸ“ð±ð”ð³ðŸŽ×ŸÒ»ðŸ“º\\x1fÐ¥ðŸ‡©ðŸ‡«ä¸€ä»¥ð“²á»‡ðŸ˜ºÐžðŸš€Ø¡Ø´Ø¥ðŸä¸‹á¸·à¼½à¼¼ðŸ“£ðŸŽ„ç³»å…³ã€‹â‰â”ðŸ‘ðŸš“ðŸ’žá½á¾¶\\x9fðŸ‘¶ðŸ‘…×˜á½¸Ê³ÑðŸŽƒðŸš½ðŸ’—ðŸ»ðŸŽ†ðŸŒ¹×›Ð•Ù‰ØµðŸ¶\\uf032\\uf02f\\uf070Ì›ââŽðŸ‡¾á¼„á¿·ðŸ™‡áƒšã€ŠðŸŒˆðŸ‘¼ðŸ‡»ï½‹ï½‡ðŸ˜¼ð‘­ðŸ»ðŸŽ¨âŠ˜ðŸ’’Î’Ú©ð˜¹ð˜–ð˜ðŸ¤šðŸ”¹á½€Ö€Õ¥Ï‡á¼¡á¼ˆð—œð’€\\ue600\\uf410\\ue607\\uf469\\uf099\\uf202\\ue608\\uf222\\uf09a\\uf203Ú†ðŸ²ðŸ—\\x0bã£ðŸ‘£ç‰ˆæŸ¥å‡»ç‚¹ðŸ›³ðŸŽðŸŽˆðŸ„á§áƒá”­ðŸ‡°ð˜¾ð’‹ç™¾çŸ¥ðŸ’¡ð•¸ð–„ð–‡å‡¸í’ˆì•½ìºë¦¬ì§€ë“¤ì œìƒê°€ë¤¼íŠ¸íƒ±ìŠ¤ì¥èƒœç¨£è€¶è®©ç»å£«ä¼ å·±è‡ªé—®å¦‚å¯¹ä¸ºðŸ¹â›“ðŸŽ»å¤šðŸ²ðŸ¯ð“€ð“ˆç¤¾å¼æ ªãƒ­ãƒãƒãƒ«ãƒžðŸ³ð ðŸï·»ð™…ð™‹ð™†ð™Š×¤×¡åˆ°å¯“å…¬ðŸŽ¾Ð›â‚½Ì¯â˜˜ð“¬à«€àª¤àª¾àª°àªœà«àª—Ñ–Éœáµ»ðŸ’¸Í¦ðŸ¤˜ðªðŸŒ…Ñ‘ðŸ’“ï¬ƒð‡ð‘Üá§Î¤\\u2007Íºá´¦Õ½Ï–ð—ªç»™ðŸŒðŸ‡¬â¬¯â¬­âš†âš²ðŸ–’ðŸ‘©æžå¥½è¯åœ¨ðŸ€ÐšðŸˆåˆ«ðŸºðŸŽ…å­ðŸ—‘\\x9cã‚µðŸ‡¼à¤®à¥à¤°à¤•ðŸ€\\uf04cðŸŽ“Íáµ’ðŸ°ðŸ™…ðŸ–\\x95ðŸ¿åœ°ð˜ŠðŸ™†ðŸ½ðŸ‘»ï¼¨á¼´â›½×‘ØºðŸŒ™\\uf029\\uf061\\uf02d\\uf028\\uf031\\uf03d\\uf071á½ á¼•á´—â’â’Œâ’‹â’Šâ’‰â’ˆá‹ŽáŠ ðŸ•ŠååðŸ•ðŸ’ð™¯ðŸŽ­á½–á¼…á¼‘á½…á½¡á¿³ðŸ¢ã‚‰ãªã†ã‚ˆã•ðŸ¥œð—·ð— ð—Ÿð—¸á¼±ðŸ’³ðŸ¡\\U0001f92a\\U0001f92fðŸ¦Ú¡\\uf10aðŸðŸŠðŸ”­ðŸ¦ðŸŸðŸ“‰ð—¤ð—˜ð—­ð—žðŸ‡²ðŸ‡­ðŸ°ðŸ•·ðŸŽŽðŸ¤–ðŽðŸŒ‘ðŸš«ðŸ‘ˆðŸ“ðŸ¥˜ðŸ–¤ðŸ–•ðŸš´È»ðˆðŸ‡±Î•Î‘ÎÎŸðŸ’¢ð˜«ð˜²ðŸ’…ðŸ‚ðŸŽð’è‡³å†¬Õ±Õ¤Ö‚Õ¶Õ«ÕµÕ¼Õ´ÕÏˆâ¤ðŸŠð—§ð—¦ð—™ðŸŒ¤å±ˆç†å±å½è™šã‚·è£½ç‡»ðŸ’ŠÑ—ä»¶äº‹ãƒªé‰„â°ðŸ‘â²â²£êœ¥ðŸ’ðŸ•‰ðŸ‘³ðŸ‚ðŸðŸ˜—ðŸ©ðŸ¦ðŸ¤³ÎžðŸŽ¥ðŸ’­ï¼§ï¼®ð‘§ð‘¦ð‘¥ðŸ­ðŸŒðŸŽ°ðŸ’ŸðŸ”¨á£á“€á‘Žá‘­á¸á‘²á“‚á“ƒá¨á‘³á¦á’§á“‡ðŸ‘¾á¹‘á½¼ð˜½ðŸ’ð‘³\\uf005å½¼ðŸ‘‘ð–Ÿð–‚ð•¿ð•¾á¼ŒðŸ—¯ðŸ’²ðŸ‘ìš©ì‚¬ëœí—ˆì‹œë“œë°˜ëŠ”ë•Œê°ˆë¶„ì„±ëŸ°ì˜ê¸ˆí™”ì™€ì´ˆëŒ€ì„œê´€ìž˜ë¡ì•Šë§Œê²Œë µí™©ê²½ì¸ê²©ìœ ì„ðŸ™‹ðŸè®¨æ£€åˆ¤æ‰¹ðŸŽâ›‘ðŸ¤’ðŸŽ¸â™¾è¿˜æ ·è¿™è°“æ —ä¸”æƒ§æå­©å©šç»“å ‚å…¨æŠŠåœ£å› å€™æ—¶é¢˜æ²»æ”¿äº›èŠä»¬å½“ä½†æ€ªæ­»æ´»å¤ä¿¡ç›¸æ›¾å¾’ç£åŸºè¯†è®¤å”¯ç§¯çŒœç”¨æŽ¥ç›´è±¡å…¶ä¸Žè´ºç¥è°Šå‹ðŸ¦†ðŸ·ðŸŽ¹ðŸ½ä¼¦ðŸ®ð™Ÿð™ð“Žð’¸ð“Œð’»ð“…ð“Šð“ð’©ð˜¿ã…“ã„¸êµ­í•œâ¦ð—¢æ‹·æ–¤é”ŸðŸ‘‡ðŸ—ðŸ•ðŸ‘ðŸ–ðŸðŸ³ðŸš—áº½é¢æ€’æ„¤é¬¼ç™½å¾—èµ¢ð™æ–¯æ‹‰é˜¿è¿Žæ¬¢æ“ðŸš‘ðŸ„å—ç‰©å® å…»â›¸ðŸ’âŽŒâ€’\\ue807ï´¿ï´¾ð“¼ð“½ð“®ð“´ð“»ðŸš¢ðŸ‘ƒðŸ‘‚í‹°íŒ¨á¼¹ðŸš¶ð€â„‹ðŸŒ„ðŸŒ‹ðŸ‹áŽ«ã€”à±¦á—žÔœð‚ð£ð™á´‘Ð†à¯¦à»á‘¯á´¨Æ„Æ½àµ¦É©\\u2001Õ°áŽ¥Ò¯\\u2000ð˜…ð˜‡çŒ´é¸¡æ€Ð¨ðŸ‡§âœ€æ–°âš­å®¶å¤©ðŸšªðŸŽŠðŸŽ‡ðŸ¥‚ðŸ¸ðŸ•ºè¯­æ±‰é€šæ™®ç”Ÿå‡ºÑ›Ñ™Ð–ìš”ì„¸í•˜ì˜ì•ˆð“µð“¿ð“’ðŸ®âå›­æ˜Žåœ†ç¾¤æˆ¦ä½œæ®Šç‰¹á½»ðŸ–‘Ê²ðŸ¤´å…‹æ²¹Ø¢éª—åé™è®®åŽå“¥æ¸©ÙŽÙå¤–ã‚¹ã‚¯ãƒ¬ðŸŽ²è‹±è”¡à¤¾ðŸ‘ ðŸ¡\\ue613ðŸ‡ðŸ“šðŸ””á¼¸éƒ½æˆðŸ”›ðŸ‘²ðŸ”¤ðŸ”„ðŸ†•×¦×š× ×£\\ue602\\x13áµ˜áµ‰Ê·á´ºá´·á´¼Ê¸Ë¢â›²ðŸŒ ä¼Žèˆžæ­ŒðŸ”—è°·ç„ðŸ‡ðŸ°ðŸ¾Ø·â›ºá¿ƒðŸŽ¯ðŸš„ðŸšŒéšœæ•…ã‚¸ã‚¨\\uf0e0\\uf818\\ue014'\n",
    "remove_dict={ord(c):\"\" for c in symbols_to_delete}\n",
    "\n",
    "# https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "def remove_url(x):\n",
    "    # it is for removing url from the text..\n",
    "    x=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE)\n",
    "    x=re.sub(r'http\\S+','',x,flags=re.MULTILINE)\n",
    "    return x\n",
    "\n",
    "def replace_puncts_mapping(x):\n",
    "    for i in puncts_mapping.keys():\n",
    "        x=x.replace(i,puncts_mapping[i])\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def replace_contraction_mapping(x):\n",
    "    for contract in contraction_mapping.keys():\n",
    "        x=x.replace(contract,contraction_mapping[contract])\n",
    "        x=x.replace(contract.capitalize(),contraction_mapping[contract].capitalize())\n",
    "#         x=x.replace(contract.upper(),contraction_mapping[contract].upper())\n",
    "        \n",
    "    return x\n",
    "\n",
    "# https://dictionary.cambridge.org/grammar/british-grammar/writing/contractions\n",
    "# preprocessing for nouns\n",
    "def fix_noun_contractions(x):\n",
    "    x=x.replace(\"'s\",\"\")\n",
    "\n",
    "    return x\n",
    "\n",
    "def fix_spellings(x):\n",
    "    for mispelled_word in mispelled_dict:\n",
    "        x=x.replace(mispelled_word,mispelled_dict[mispelled_word])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def replace_puncts(x):\n",
    "    for p in puncts: \n",
    "        x=x.replace(p,f' {p} ')          # adding spaces to desirerd puncts\n",
    "#         x=x.replace(p,\"\")            # removing the puncts from the vocab..\n",
    "    \n",
    "    x=x.translate(remove_dict)\n",
    "    return x\n",
    "\n",
    "\n",
    "train['comment_text']=train['comment_text'].progress_apply(remove_url)\n",
    "train['comment_text']=train['comment_text'].progress_apply(replace_puncts_mapping)       \n",
    "train['comment_text']=train['comment_text'].progress_apply(replace_contraction_mapping)\n",
    "train['comment_text']=train['comment_text'].progress_apply(fix_noun_contractions)\n",
    "train['comment_text']=train['comment_text'].progress_apply(fix_spellings)\n",
    "train['comment_text']=train['comment_text'].progress_apply(replace_puncts)\n",
    "# train_sentences=train['comment_text'].progress_apply(sen)\n",
    "\n",
    "test['comment_text']=test['comment_text'].progress_apply(remove_url)\n",
    "test['comment_text']=test['comment_text'].progress_apply(replace_puncts_mapping)\n",
    "test['comment_text']=test['comment_text'].progress_apply(replace_contraction_mapping)\n",
    "test['comment_text']=test['comment_text'].progress_apply(fix_noun_contractions)\n",
    "test['comment_text']=test['comment_text'].progress_apply(fix_spellings)\n",
    "test['comment_text']=test['comment_text'].progress_apply(replace_puncts)\n",
    "# test_sentences=test_sentences.progress_apply(sen)\n",
    "\n",
    "# vocab=build_vocab(train_sentences)\n",
    "# oov=check_coverage(vocab,embeddings_dict_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZING AND PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of vocabulary is 369892\n",
      "Padding and truncating\n",
      "6\n",
      "Training Shape (1804874, 250) (1804874, 7)\n",
      "Test Shape (97320, 250)\n",
      "CPU times: user 3min 57s, sys: 2.9 s, total: 4min\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_words=300000              # 1 lakh top words (vocab nearly contains 4 lakh words)\n",
    "max_len=250\n",
    "embed_dim=300\n",
    "\n",
    "# I am keeping filters as default of the keras...\n",
    "tokenizer=Tokenizer(num_words=max_words,lower=False)\n",
    "tokenizer.fit_on_texts(list(train['comment_text']))\n",
    "print(\"The length of vocabulary is\",len(tokenizer.word_index))\n",
    "\n",
    "X=tokenizer.texts_to_sequences(list(train['comment_text'].values))\n",
    "X_test=tokenizer.texts_to_sequences(list(test['comment_text'].values))\n",
    "\n",
    "# padding and truncating\n",
    "print(\"Padding and truncating\")\n",
    "X=np.array(pad_sequences(X,maxlen=max_len,padding='pre',truncating='pre',dtype='int32'),dtype='int32')\n",
    "X_test=np.array(pad_sequences(X_test,maxlen=max_len,padding='pre',truncating='pre'),dtype='int32')\n",
    "\n",
    "# for i in ['severe_toxicity','obscene','identity_attack','insult','threat']:\n",
    "#     train[i]=(train[i]>=0.5).astype(int)\n",
    "\n",
    "# defining auxilary training\n",
    "aux_cols=['target','severe_toxicity','obscene','identity_attack','insult','threat']\n",
    "print(len(aux_cols))\n",
    "y_aux=train[aux_cols].values\n",
    "train['target']=(train['target']>=0.5).astype(int)\n",
    "y=train['target'].values\n",
    "\n",
    "y=np.hstack((y.reshape(-1,1),y_aux))\n",
    "\n",
    "print(\"Training Shape\",X.shape,y.shape)\n",
    "print(\"Test Shape\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDING MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe embedding\n",
      "Number of embeddings loaded: 2196017\n",
      "The Shape of the glove matrix is (300000, 300)\n",
      "Extracting Fast Text embedding\n",
      "Number of embeddings loaded: 2000000\n",
      "The Shape of the fasttext matrix is (300000, 300)\n",
      "Extracting Fast Text SubWord Embedding\n",
      "Number of embeddings loaded: 2000000\n",
      "The Shape of the fasttext subword matrix is (300000, 300)\n",
      "Extracting fasttext wikinews embedding\n",
      "Number of embeddings loaded: 1000000\n",
      "The Shape of the wikinews matrix is (300000, 300)\n",
      "CPU times: user 54.5 s, sys: 9.62 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def give_embed_matrix(word_index,embeddings_dict):\n",
    "    nb_words=min(max_words,len(word_index)+1)\n",
    "    embeddings_matrix = np.zeros((nb_words, embed_dim))\n",
    "    for word,index in word_index.items():\n",
    "        if index>=max_words:\n",
    "            continue\n",
    "        # implies that word has embedding\n",
    "        if embeddings_dict.get(word) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(word)\n",
    "        \n",
    "        elif embeddings_dict.get(word.lower()) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(word.lower())\n",
    "        \n",
    "        elif embeddings_dict.get(word.upper()) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(word.upper())\n",
    "        \n",
    "        elif embeddings_dict.get(word.capitalize()) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(word.capitalize())\n",
    "            \n",
    "        elif embeddings_dict.get(ps.stem(word)) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(ps.stem(word))\n",
    "            \n",
    "        elif embeddings_dict.get(lc.stem(word)) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(lc.stem(word))\n",
    "        \n",
    "        elif embeddings_dict.get(sb.stem(word)) is not None:\n",
    "            embeddings_matrix[index] = embeddings_dict.get(sb.stem(word))\n",
    "            \n",
    "    return embeddings_matrix\n",
    "\n",
    "print(\"Extracting GloVe embedding\")\n",
    "with open(\"../input/pickledglove300d22mforkernelcompetitions/glove.2M.840B.300d.pkl\",'rb') as f:\n",
    "    embeddings_dict_glove = pickle.load(f)\n",
    "print(\"Number of embeddings loaded:\",len(embeddings_dict_glove))\n",
    "\n",
    "# print(\"Extracting GloVe embedding\")\n",
    "# with open(\"../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\",'rb') as f:\n",
    "#     embeddings_dict_glove = pickle.load(f)\n",
    "# print(\"Number of embeddings loaded:\",len(embeddings_dict_glove))\n",
    "\n",
    "embeddings_matrix_glove=give_embed_matrix(tokenizer.word_index,embeddings_dict_glove)\n",
    "print(\"The Shape of the glove matrix is\",embeddings_matrix_glove.shape)\n",
    "\n",
    "gc.enable()\n",
    "del embeddings_dict_glove\n",
    "gc.collect()\n",
    "\n",
    "print(\"Extracting Fast Text embedding\")\n",
    "with open(\"../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\",'rb') as f:\n",
    "    embeddings_dict_fasttext = pickle.load(f)\n",
    "print(\"Number of embeddings loaded:\",len(embeddings_dict_fasttext))\n",
    "\n",
    "embeddings_matrix_fasttext=give_embed_matrix(tokenizer.word_index,embeddings_dict_fasttext)\n",
    "print(\"The Shape of the fasttext matrix is\",embeddings_matrix_fasttext.shape)\n",
    "\n",
    "gc.enable()\n",
    "del embeddings_dict_fasttext\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"Extracting Fast Text SubWord Embedding\")\n",
    "with open(\"../input/pickled-fasttext-subword/fasttext_subword.2M.600B.300d.pkl\",\"rb\") as f:\n",
    "    embeddings_dict_fasttext_subword = pickle.load(f)\n",
    "print(\"Number of embeddings loaded:\",len(embeddings_dict_fasttext_subword))\n",
    "\n",
    "embeddings_matrix_fasttext_subword = give_embed_matrix(tokenizer.word_index,embeddings_dict_fasttext_subword)\n",
    "print(\"The Shape of the fasttext subword matrix is\",embeddings_matrix_fasttext_subword.shape)\n",
    "\n",
    "gc.enable()\n",
    "del embeddings_dict_fasttext_subword\n",
    "gc.collect()\n",
    "\n",
    "print(\"Extracting fasttext wikinews embedding\")\n",
    "with open(\"../input/pickled-fasttext-wikinews/wikinews.1M.16B.300d.pkl\",\"rb\") as f:\n",
    "    embeddings_dict_wikinews = pickle.load(f)\n",
    "print(\"Number of embeddings loaded:\",len(embeddings_dict_wikinews))\n",
    "\n",
    "embeddings_matrix_wikinews = give_embed_matrix(tokenizer.word_index,embeddings_dict_wikinews)\n",
    "print(\"The Shape of the wikinews matrix is\",embeddings_matrix_wikinews.shape)\n",
    "\n",
    "gc.enable()\n",
    "del embeddings_dict_wikinews\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Embeddings matrix shape (300000, 300)\n"
     ]
    }
   ],
   "source": [
    "# combining the embedding matrices \n",
    "embeddings_matrix = 0.3*embeddings_matrix_glove + 0.3*embeddings_matrix_fasttext + \\\n",
    "                    0.3*embeddings_matrix_fasttext_subword + 0.1*embeddings_matrix_wikinews\n",
    "\n",
    "\n",
    "# embeddings_matrix=np.hstack((embeddings_matrix_glove,embeddings_matrix_fasttext,\\\n",
    "#                                      embeddings_matrix_fasttext_subword))\n",
    "\n",
    "gc.enable()\n",
    "del embeddings_matrix_glove,embeddings_matrix_fasttext,embeddings_matrix_fasttext_subword\n",
    "gc.collect()           \n",
    "\n",
    "\n",
    "# updating the embedding dimension\n",
    "embed_dim=embeddings_matrix.shape[1]\n",
    "print(\"Final Embeddings matrix shape\",embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving the tokenizer and embeddings matrix for testing....\n",
    "with open(\"tokenizer.pkl\",\"wb\") as handle:\n",
    "    pickle.dump(tokenizer.word_index,handle)\n",
    "    \n",
    "# saving the embeddings matrix\n",
    "np.save(\"embeddings_matrix.npy\",embeddings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from the kernel https://www.kaggle.com/dborkan/benchmark-kernel\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "# making subgroups as 1 and 0 with threshold 0.5\n",
    "# the nan value in some examples \n",
    "for subgroup in identity_columns:\n",
    "    train[subgroup]=(train[subgroup]>=0.5).astype(np.int8)\n",
    "\n",
    "def auc_score(y_true,y_pred):\n",
    "    return roc_auc_score(y_true,y_pred)\n",
    "\n",
    "def compute_bpsn(df,subgroup):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\n",
    "    Here, we restrict the test set to the non-toxic examples that mention the identity and \n",
    "    the toxic examples that do not\"\"\"\n",
    "    \n",
    "    subgroup_negative_examples=df.loc[(df[subgroup]==1) & (df['target']==0)]\n",
    "    non_subgroup_positive_examples=df.loc[(df[subgroup]==0)&(df['target']==1)]\n",
    "    examples=pd.concat([subgroup_negative_examples,non_subgroup_positive_examples])\n",
    "    \n",
    "    return roc_auc_score(examples['target'].values,examples['preds'].values)\n",
    "    \n",
    "    \n",
    "def compute_bnsp(df,subgroup):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\n",
    "    Here, we restrict the test set to the toxic examples that mention the identity and \n",
    "    the non-toxic examples that do not.\"\"\"\n",
    "    subgroup_positive_examples=df.loc[(df[subgroup]==1)&(df['target']==1)]\n",
    "    non_subgroup_negative_examples=df.loc[(df[subgroup]==0)&(df['target']==0)]\n",
    "    examples=pd.concat([subgroup_positive_examples,non_subgroup_negative_examples])\n",
    "    \n",
    "    return roc_auc_score(examples['target'].values,examples['preds'].values)\n",
    "\n",
    "def compute_bias_auc(indices=None,preds=None,df=None):\n",
    "    \"\"\" Computes the three auc for all the subgroups \"\"\"\n",
    "    if df is None:\n",
    "        df=train.copy()\n",
    "        df['preds']=preds\n",
    "        df=df.loc[indices].copy()\n",
    "    \n",
    "    records=[]\n",
    "    for subgroup in identity_columns:\n",
    "        record={\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': df.loc[df[subgroup]==1].shape[0]\n",
    "        }\n",
    "        record[SUBGROUP_AUC]=roc_auc_score(df.loc[df[subgroup]==1,'target'].values,\n",
    "                                                       df.loc[df[subgroup]==1,'preds'].values)\n",
    "        record[BPSN_AUC]=compute_bpsn(df,subgroup)\n",
    "        record[BNSP_AUC]=compute_bnsp(df,subgroup)\n",
    "        records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(records)[['subgroup','subgroup_size',SUBGROUP_AUC,BPSN_AUC,BNSP_AUC]]\n",
    "\n",
    "def compute_power_mean(series,p):\n",
    "    total=np.sum(np.power(series,p))\n",
    "    return np.power(total/len(series),1/p)\n",
    "\n",
    "def compute_final_metric(indices,preds,overall_auc=None,p=-5,w=0.25):\n",
    "    df=train.copy()\n",
    "    df=df.loc[indices].copy()\n",
    "    df['preds']=preds\n",
    "    \n",
    "    if overall_auc is None:\n",
    "        overall_auc=roc_auc_score(df['target'],df['preds'])\n",
    "    bias_df=compute_bias_auc(df=df)\n",
    "    bias_score=np.average([\n",
    "        compute_power_mean(bias_df[SUBGROUP_AUC],p),\n",
    "        compute_power_mean(bias_df[BPSN_AUC],p),\n",
    "        compute_power_mean(bias_df[BNSP_AUC],p),\n",
    "    ])\n",
    "    \n",
    "    return bias_df,w*overall_auc+(1-w)*bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80230672, 0.80230672, 0.80230672, 0.80230672, 1.60461343,\n",
       "       1.60461343, 0.80230672, 0.80230672, 0.80230672, 0.80230672])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # preds=np.random.uniform(0,1,size=(train.shape[0],))\n",
    "# preds=1/(1+np.exp(-np.random.normal(0,1,size=(train.shape[0],))))\n",
    "# print(preds.shape)\n",
    "# bias_df,score=compute_final_metric(train.index,preds)\n",
    "# print(score)\n",
    "# display(bias_df)\n",
    "\n",
    "# taken from the kernel https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution\n",
    "weights=np.ones((train.shape[0],))\n",
    "weights = weights + train[identity_columns].sum(axis=1).astype(np.bool).astype(np.int).values\n",
    "\n",
    "# Background Positive, Subgroup Negative\n",
    "weights = weights + (( (train['target']>=0.5).astype(np.bool).astype(np.int) +  \\\n",
    "        (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(np.int).astype(np.bool) )>1)\\\n",
    "                    .astype(np.bool).astype(np.int)\n",
    "\n",
    "# Background Negative, Subgroup Positive\n",
    "weights = weights + (( (train['target']<0.5).astype(np.bool).astype(np.int) + \\\n",
    "        (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(np.int).astype(np.bool) )>1)\\\n",
    "                    .astype(np.bool).astype(np.int)\n",
    "\n",
    "# weights = np.ones((train.shape[0],))\n",
    "# weights = weights + train[identity_columns].sum(axis=1).values\n",
    "# # bpsn\n",
    "# weights = weights + (train['target']<0.5)*(train[identity_columns].fillna(0).values>=0.5).sum(axis=1) + \\\n",
    "#                     (train['target']>=0.5)*(train[identity_columns].fillna(0).values<0.5).sum(axis=1)\n",
    "\n",
    "# # bnsp\n",
    "# weights = weights + (train.loc[train['target']>=0.5][identity_columns].fillna(0).values>=0.5).sum(axis=1) + \\\n",
    "#                     (train.loc[])\n",
    "\n",
    "weights=(weights.values)/np.mean(weights)\n",
    "weights[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,hidden_dim,max_len):\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.max_len=max_len\n",
    "        \n",
    "        self.tanh=nn.Tanh()\n",
    "        self.linear=nn.Linear(in_features=self.hidden_dim,out_features=1,bias=False)\n",
    "        self.softmax=nn.Softmax(dim=1)        \n",
    "        \n",
    "    def forward(self,h):\n",
    "        \n",
    "        m=self.tanh(h)\n",
    "        \n",
    "        alpha=self.linear(m)\n",
    "        \n",
    "        alpha=torch.squeeze(alpha)           # shape of alpha will be batch_size*max_len\n",
    "        \n",
    "#         print(\"Alpha shape:\",alpha.shape)\n",
    "        \n",
    "        # softmax(note that softmax is along dimension 1)\n",
    "        alpha=self.softmax(alpha)\n",
    "        \n",
    "        # unsequezzing alpha to get shape as batch_size*max_len*1\n",
    "        alpha=torch.unsqueeze(alpha,-1)\n",
    "        \n",
    "        # we have to define r\n",
    "        r=h*alpha\n",
    "        \n",
    "        # now we have to take sum and shape of r is batch_size*hidden_size\n",
    "        r=torch.sum(r,dim=1)\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Module):\n",
    "    def __init__(self,p=0.2):\n",
    "        super(SpatialDropout,self).__init__()\n",
    "        self.p = p\n",
    "        self.dropout2d = nn.Dropout2d(p=self.p)\n",
    "    \n",
    "    def forward(self,embeddings):\n",
    "        embeddings = torch.transpose(embeddings,1,2)  # (batch_size,embed_size,timestep)\n",
    "        embeddings = torch.unsqueeze(embeddings,-1)  # (batch_size,embed_size,timestep,1)\n",
    "        embeddings = self.dropout2d(embeddings)      # (batch_size,embed_size,timstep,1)\n",
    "        embeddings = torch.squeeze(embeddings)      # (batch_size,embed_size,timestep)\n",
    "        embeddings = torch.transpose(embeddings,1,2) # (batch_size,timestep,embed_size)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# model params\n",
    "hidden_units=64\n",
    "class LstmGru(nn.Module):\n",
    "    def __init__(self,embeddings_matrix,num_aux_targets):\n",
    "        super(LstmGru,self).__init__()\n",
    "        \n",
    "        self.embedding=nn.Embedding.from_pretrained(torch.Tensor(embeddings_matrix),freeze=True)\n",
    "        self.embedding_dropout = SpatialDropout(0.1)\n",
    "\n",
    "#         self.embedding_linear = nn.Linear(in_features = embed_dim,out_features = 128)\n",
    "#         self.embedding_relu = nn.ReLU()\n",
    "        self.lstm=nn.LSTM(input_size=embed_dim,hidden_size=hidden_units,\n",
    "                              bidirectional=True,batch_first=True)\n",
    "        \n",
    "        # gru output goes to lstm\n",
    "        self.gru=nn.GRU(input_size=2*hidden_units,hidden_size=hidden_units,\n",
    "                           bidirectional=True,batch_first=True)\n",
    "        \n",
    "        # lstm attention\n",
    "        self.lstm_attention=Attention(2*hidden_units,max_len)\n",
    "        \n",
    "        # gru attention\n",
    "        self.gru_attention=Attention(2*hidden_units,max_len)\n",
    "        \n",
    "        self.linear1=nn.Linear(in_features=8*hidden_units,out_features=32)\n",
    "        self.batch1=nn.BatchNorm1d(32)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.drop1=nn.Dropout(0.25)\n",
    "        \n",
    "        self.linear2=nn.Linear(in_features=32,out_features=1)\n",
    "        self.linear3=nn.Linear(in_features=32,out_features=num_aux_targets)\n",
    "        \n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        batch_size=X.shape[0]\n",
    "        \n",
    "        embeds=self.embedding(X.long())\n",
    "        \n",
    "        embeds=self.embedding_dropout(embeds)\n",
    "        \n",
    "#         embeds = self.embedding_relu(self.embedding_linear(embeds))\n",
    "        \n",
    "        h_lstm,_=self.lstm(embeds)\n",
    "        h_gru,_=self.gru(h_lstm)\n",
    "        \n",
    "        # max pooling over time\n",
    "        h_lstm_max,_=torch.max(h_lstm,1)\n",
    "        h_gru_max,_=torch.max(h_gru,1)\n",
    "        \n",
    "        # mean average pooling over time\n",
    "        h_lstm_mean=torch.mean(h_lstm,1)\n",
    "        h_gru_mean=torch.mean(h_gru,1)\n",
    "        \n",
    "        # attention\n",
    "        h_lstm_attend=self.lstm_attention(h_lstm)\n",
    "        h_gru_attend=self.gru_attention(h_gru)\n",
    "        \n",
    "        h=torch.cat((h_lstm_attend,h_gru_attend,h_lstm_max,h_gru_max),dim=1)\n",
    "        \n",
    "        output=self.relu1(self.batch1(self.linear1(h)))\n",
    "        output=self.drop1(output)\n",
    "        \n",
    "        linear_output=self.sigmoid(self.linear2(output))\n",
    "        aux_output=self.sigmoid(self.linear3(output))\n",
    "        \n",
    "        output=torch.cat([linear_output,aux_output],dim=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def initialize_model(embeddings_matrix,num_aux_targets):\n",
    "    model=LstmGru(embeddings_matrix,num_aux_targets)\n",
    "    \n",
    "    # setting all the dtypes to float\n",
    "    model.float()\n",
    "    \n",
    "    # pushing the code to gpu\n",
    "    model.cuda()\n",
    "    \n",
    "    # params\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"Total trainiable Param's are\",trainable_params)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainiable Param's are 278855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LstmGru(\n",
       "  (embedding): Embedding(300000, 300)\n",
       "  (embedding_dropout): SpatialDropout(\n",
       "    (dropout2d): Dropout2d(p=0.1)\n",
       "  )\n",
       "  (lstm): LSTM(300, 64, batch_first=True, bidirectional=True)\n",
       "  (gru): GRU(128, 64, batch_first=True, bidirectional=True)\n",
       "  (lstm_attention): Attention(\n",
       "    (tanh): Tanh()\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "  (gru_attention): Attention(\n",
       "    (tanh): Tanh()\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (softmax): Softmax()\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (batch1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (drop1): Dropout(p=0.25)\n",
       "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (linear3): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_model(embeddings_matrix,y_aux.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BceLoss(nn.Module):\n",
    "    def __init__(self,eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self,y_true,y_pred,weights=None):\n",
    "        \"\"\"\n",
    "        Computes the BCE Loss with respect to y_true\n",
    "        Weights is of shape : (batch_size,)\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = torch.ones((y_true.shape[0],)).type('torch.FloatTensor').cuda()\n",
    "        \n",
    "        y_pred = torch.clamp(y_pred,self.eps,1-self.eps)\n",
    "        m = y_pred.shape[0]\n",
    "        \n",
    "        if len(y_true.shape)==1:\n",
    "            # changing y_true and y_pred \n",
    "            y_true = torch.unsqueeze(y_true,1)\n",
    "            y_pred = torch.unsqueeze(y_pred,1)\n",
    "        loss = torch.sum(y_true*torch.log(y_pred)+(1-y_true)*torch.log(1-y_pred),dim=1)\n",
    "#         print(y_true.shape,y_pred.shape,loss.shape)\n",
    "        loss = -torch.sum(weights*loss)/m\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class MseLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,y_true,y_pred,weights=None):\n",
    "        if weights is None:\n",
    "            weights = torch.ones((y_true.shape[0],)).type('torch.FloatTensor').cuda()\n",
    "            \n",
    "        m = y_pred.shape[0]\n",
    "        \n",
    "        if len(y_true.shape)==1:\n",
    "            # changing y_true and y_pred \n",
    "            y_true = torch.unsqueeze(y_true,1)\n",
    "            y_pred = torch.unsqueeze(y_pred,1)\n",
    "        \n",
    "        loss = torch.sum(torch.pow(y_true-y_pred,2),dim=1)\n",
    "        loss = torch.sum(weights*loss)/m\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self,gamma=0,eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma=gamma\n",
    "        self.eps=eps\n",
    "    \n",
    "    def forward(self,y_true,y_pred,weights=None):\n",
    "        if weights is None:\n",
    "            weights = torch.ones((y_true.shape[0],)).type('torch.FloatTensor').cuda()\n",
    "\n",
    "        y_pred = torch.clamp(y_pred,self.eps,1-self.eps)\n",
    "        m = y_pred.shape[0]\n",
    "        \n",
    "        if len(y_true.shape)==1:\n",
    "            # changing the shape of y_pred and y_true\n",
    "            y_true = torch.unsqueeze(y_true,1)\n",
    "            y_pred = torch.unsqueeze(y_pred,1)\n",
    "        \n",
    "        loss = torch.sum(y_true*torch.pow(1-y_pred,self.gamma)*torch.log(y_pred) + \\\n",
    "               (1-y_true)*torch.pow(y_pred,self.gamma)*torch.log(1-y_pred),dim=1)\n",
    "        \n",
    "        loss = -torch.sum(weights*loss)/m\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self,gamma = 0):\n",
    "        super(CustomLoss,self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.focal_loss = BinaryFocalLoss(gamma=self.gamma)\n",
    "        self.bce_loss = BceLoss()\n",
    "        \n",
    "    def forward(self,y_true,y_pred,weights=None):\n",
    "        return self.focal_loss(y_true,y_pred,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CYCLIC LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOME USEFUL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_val(model,val_iterator,m_val):\n",
    "    \"\"\"\n",
    "        model : pytorch model.\n",
    "        val_iterator : validation dataset iterator.\n",
    "        m_val : number of validation examples.\n",
    "    \"\"\"\n",
    "    # setting up the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    val_targets=np.zeros((m_val,))\n",
    "    val_preds=np.zeros((m_val,))\n",
    "    val_index=0\n",
    "    \n",
    "    start=datetime.datetime.now()\n",
    "\n",
    "    for batch,(X_val,y_val) in enumerate(val_iterator):\n",
    "        X_val=Variable(X_val.cuda())\n",
    "        y_val=Variable(y_val.type('torch.FloatTensor').cuda())\n",
    "\n",
    "        y_pred=model.forward(X_val)\n",
    "\n",
    "        # appending the preds\n",
    "        val_preds[val_index:val_index+X_val.shape[0]]=y_pred[:,0].cpu().detach().numpy().reshape((-1,))\n",
    "\n",
    "        # appending the targets\n",
    "        val_targets[val_index:val_index+X_val.shape[0]]=y_val[:,0].cpu().detach().numpy().reshape((-1,))\n",
    "\n",
    "        # updating the val_index\n",
    "        val_index=val_index+X_val.shape[0]\n",
    "\n",
    "        # printing\n",
    "        logger=str(val_index)+\"/\"+str(m_val)\n",
    "        if batch<len(val_iterator)-1:\n",
    "            print(logger,end='\\r')\n",
    "        else:\n",
    "            print(logger,end=\" \")\n",
    "            \n",
    "    end = datetime.datetime.now()\n",
    "    print(\"Predictions done on validation data in\",round((end-start).total_seconds()),\"seconds\")\n",
    "        \n",
    "    return val_targets,val_preds\n",
    "\n",
    "def predict_on_test(model,test_iterator,m_test):\n",
    "    \n",
    "    # model at evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_preds = np.zeros((m_test,))\n",
    "    test_targets = np.zeros((m_test,)) \n",
    "    test_index=0\n",
    "    \n",
    "    start=datetime.datetime.now()\n",
    "    \n",
    "    for batch,(X_test,y_test) in enumerate(test_iterator):\n",
    "        X_test=Variable(X_test.cuda())\n",
    "        \n",
    "        y_pred=model.forward(X_test)\n",
    "        # appending the preds\n",
    "        test_preds[test_index:test_index+X_test.shape[0]]=y_pred[:,0].cpu().detach().numpy().reshape((-1,))\n",
    "    \n",
    "        test_index=test_index+X_test.shape[0]\n",
    "    \n",
    "        logger=str(test_index)+\"/\"+str(m_test)\n",
    "    \n",
    "        if batch<len(test_iterator)-1:\n",
    "            print(logger,end='\\r')\n",
    "        else:\n",
    "            print(logger,end=\" \")\n",
    "        \n",
    "    end=datetime.datetime.now()\n",
    "    print(\"Predictions done on test data in\",round((end-start).total_seconds()),\"seconds\")\n",
    "\n",
    "    return test_targets,test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(model,optimizer,loss_fn,scheduler=None,train_iterator=None,\n",
    "             val_iterator=None,m_train=None,m_val=None,epochs=None,\n",
    "             fname=None,val_indices=None,test_iterator=None,m_test=None):\n",
    "    \"\"\"\n",
    "        model : pytroch model.\n",
    "        optimizer : any optimizer from torch.optim.\n",
    "        loss_fn : loss function or list of loss functions.\n",
    "        scheduler : learning rate scheduler.\n",
    "        m_train : number of training examples.\n",
    "        m_val : number of validation examples.\n",
    "        epochs : number of epochs.\n",
    "        fname : file name to save the model(it always saves the best with respect to validation).\n",
    "        m_test : number of test examples.\n",
    "        \n",
    "        returns\n",
    "        best train preds and best val preds(selected based on validation score).\n",
    "        and evaluations(which cotains losses,accuracy and f1 score of every epoch).\n",
    "        val_indices : index of validation examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_auc=[]\n",
    "    val_auc=[]\n",
    "    val_bias_auc=[]\n",
    "    evals={}\n",
    "    best_val_auc,best_val_bias_auc=0,0\n",
    "    val_preds_dict = {}\n",
    "    test_preds_dict = {}\n",
    "    \n",
    "    prev_rng_state = torch.get_rng_state()  # get previous rng state\n",
    "    \n",
    "    for ep_num in range(epochs):\n",
    "        print(\"Epoch\",\"{0}/{1}:\".format(ep_num+1,epochs))\n",
    "        \n",
    "        # measuring time\n",
    "        start=datetime.datetime.now()\n",
    "        \n",
    "        train_targets=np.zeros((m_train,))\n",
    "        train_preds=np.zeros((m_train,))\n",
    "        \n",
    "        train_index=0\n",
    "        \n",
    "        # setting up the model in train mode\n",
    "        model.train()\n",
    "        \n",
    "        # set rng state\n",
    "        torch.set_rng_state(prev_rng_state) \n",
    "        \n",
    "        # iterating through batch\n",
    "        for batch,(X_train,y_train,weights) in enumerate(train_iterator):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            X_train=Variable(X_train.cuda())\n",
    "            y_train=Variable(y_train.type('torch.FloatTensor').cuda())\n",
    "            weights=Variable(weights.type('torch.FloatTensor').cuda())\n",
    "            \n",
    "#             print(X_train.shape)\n",
    "            y_pred=model.forward(X_train)\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            \n",
    "            if isinstance(loss_fn,list):\n",
    "                if ep_num <= 6:\n",
    "                    loss = loss_fn[0].forward(y_train,y_pred,weights = weights)   # binary entropy loss for 7 epochs\n",
    "                    if batch == 0:\n",
    "                        print(\"Using BCE Loss\")\n",
    "                else:\n",
    "                    loss = loss_fn[1].forward(y_train,y_pred,weights = weights)  # focal loss for 7 epochs\n",
    "                    if batch == 0:\n",
    "                        print(\"Using Focal loss\")\n",
    "            else:\n",
    "                loss = loss_fn.forward(y_train,y_pred,weights = weights)\n",
    "            \n",
    "            # appedning the preds\n",
    "            train_preds[train_index:train_index+X_train.shape[0]]=y_pred[:,0].cpu().detach().numpy().reshape((-1,))\n",
    "            \n",
    "            # appending the targets\n",
    "            train_targets[train_index:train_index+X_train.shape[0]]=y_train[:,0].cpu().detach().numpy().reshape((-1,))\n",
    "            \n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_index=train_index+X_train.shape[0]\n",
    "            logger=str(train_index)+\"/\"+str(m_train)\n",
    "            if batch<len(train_iterator)-1:\n",
    "                print(logger,end='\\r')\n",
    "            else:\n",
    "                print(logger)\n",
    "                \n",
    "        \n",
    "        # prediction on validation data\n",
    "        val_targets,val_preds = predict_on_val(model,val_iterator,m_val)\n",
    "        \n",
    "        prev_rng_state = torch.get_rng_state() # save rng state\n",
    "        \n",
    "        # prediction on test data\n",
    "        test_targets,test_preds = predict_on_test(model,test_iterator,m_test)\n",
    "                \n",
    "        val_preds_dict[ep_num+1] = val_preds\n",
    "        test_preds_dict[ep_num+1] = test_preds\n",
    "        \n",
    "        # finding the loss and auc score.\n",
    "        trainloss=log_loss(train_targets,train_preds)\n",
    "        valloss=log_loss(val_targets,val_preds)\n",
    "        \n",
    "        trainauc=auc_score(train_targets,train_preds)\n",
    "        valauc=auc_score(val_targets,val_preds)\n",
    "        \n",
    "        # finding the bias auc.\n",
    "        _,valbiasauc=compute_final_metric(val_indices,val_preds)\n",
    "        \n",
    "        train_loss.append(trainloss),val_loss.append(valloss)\n",
    "        train_auc.append(trainauc),val_auc.append(valauc)\n",
    "        val_bias_auc.append(valbiasauc)\n",
    "        \n",
    "        # measuring time.\n",
    "        end=datetime.datetime.now()\n",
    "        \n",
    "        # printing. \n",
    "        print(\"Seconds = \",round((end-start).total_seconds()),end=\" \")\n",
    "        print(\"train loss = \",round(trainloss,7),end=\" \")\n",
    "        print(\"train auc = \",round(trainauc,7),end=\" \")\n",
    "        \n",
    "        print(\"val loss = \",round(valloss,7),end=\" \")\n",
    "        print(\"val auc = \",round(valauc,7),end=\" \")\n",
    "        print(\"val bias auc = \",round(valbiasauc,7))\n",
    "        \n",
    "        if(valbiasauc>best_val_bias_auc):\n",
    "            print(\"Validation Bias AUC score increased from \",np.round(best_val_bias_auc,7),\"to\",\\\n",
    "                                  np.round(valbiasauc,7),\"Saving the model at\",fname)\n",
    "            # saving without the embedding... (method is seen in the forums)\n",
    "            temp_dict = model.state_dict()\n",
    "            del temp_dict['embedding.weight']\n",
    "            torch.save(temp_dict,fname)\n",
    "            best_val_bias_auc = valbiasauc\n",
    "            best_val_auc = valauc\n",
    "            best_epoch_number = ep_num + 1           \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # outside of epoch loop.\n",
    "    evals['val_preds'] = val_preds_dict\n",
    "    evals['test_preds'] = test_preds_dict\n",
    "    \n",
    "    evals['train_loss'] = train_loss\n",
    "    evals['val_loss'] = val_loss\n",
    "    evals['train_auc'] = train_auc\n",
    "    evals['val_auc'] = val_auc\n",
    "    evals['val_bias_auc'] = val_bias_auc\n",
    "    evals['best_val_auc']=best_val_auc\n",
    "    evals['best_val_bias_auc'] = best_val_bias_auc\n",
    "    evals['best_epoch'] = best_epoch_number\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape (1804874, 250) (1804874, 7)\n",
      "Testing Shape (97320, 250)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Shape\",X.shape,y.shape)\n",
    "print(\"Testing Shape\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_dataset(X_train,y_train,batch_size=512,weights=None):\n",
    "    \"\"\"\n",
    "        weights : weights of training examples. length of m_train\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights=np.ones((y_train.shape[0],))\n",
    "    X_train,y_train,weights=torch.Tensor(X_train),torch.Tensor(y_train),torch.Tensor(weights)\n",
    "    \n",
    "    # train dataset \n",
    "    train_dataset=TensorDataset(X_train,y_train,weights)\n",
    "    \n",
    "    # Passing this to data loader\n",
    "    # shuffle set to true imples for every epoch data is shuffled \n",
    "    train_iterator=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "    \n",
    "    return train_iterator\n",
    "\n",
    "\n",
    "def make_val_dataset(X_val,y_val,batch_size=512):\n",
    "    X_val,y_val=torch.Tensor(X_val),torch.Tensor(y_val)\n",
    "    \n",
    "    # val dataset\n",
    "    val_dataset=TensorDataset(X_val,y_val)\n",
    "    \n",
    "    # Passing this to data loader\n",
    "    val_iterator=DataLoader(val_dataset,batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "    \n",
    "    return val_iterator\n",
    "\n",
    "def make_test_dataset(X_test,batch_size=512):\n",
    "    # making test data and test iterator\n",
    "    if len(y.shape) > 1 :\n",
    "        y_test = np.zeros((X_test.shape[0],y.shape[1]))\n",
    "    else:\n",
    "        y_test = np.zeros((X_test.shape[0],))\n",
    "    \n",
    "    X_test,y_test = torch.Tensor(X_test),torch.Tensor(y_test)\n",
    "    \n",
    "    # test dataset\n",
    "    test_dataset = TensorDataset(X_test,y_test)\n",
    "    \n",
    "    # passing this to data loader\n",
    "    test_iterator = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "    \n",
    "    return test_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================= FOLD 1 =============================================\n",
      "Training Shape: (1443899, 250) (1443899, 7)\n",
      "Validation Shape: (360975, 250) (360975, 7)\n",
      "Total trainiable Param's are 278855\n",
      "Epoch 1/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  285 train loss =  0.2756565 train auc =  0.9357456 val loss =  0.256332 val auc =  0.9570363 val bias auc =  0.91869\n",
      "Validation Bias AUC score increased from  0 to 0.91869 Saving the model at LSTM_GRU_fold_1_seed_42.pt\n",
      "\n",
      "\n",
      "Epoch 2/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  286 train loss =  0.2324357 train auc =  0.956701 val loss =  0.213285 val auc =  0.9633145 val bias auc =  0.9244929\n",
      "Validation Bias AUC score increased from  0.91869 to 0.9244929 Saving the model at LSTM_GRU_fold_1_seed_42.pt\n",
      "\n",
      "\n",
      "Epoch 3/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  284 train loss =  0.2223652 train auc =  0.9609399 val loss =  0.2210239 val auc =  0.9638181 val bias auc =  0.9301085\n",
      "Validation Bias AUC score increased from  0.9244929 to 0.9301085 Saving the model at LSTM_GRU_fold_1_seed_42.pt\n",
      "\n",
      "\n",
      "Epoch 4/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  285 train loss =  0.2159706 train auc =  0.9632382 val loss =  0.2004853 val auc =  0.9642993 val bias auc =  0.9300724\n",
      "\n",
      "\n",
      "Epoch 5/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  285 train loss =  0.2061063 train auc =  0.966397 val loss =  0.2071356 val auc =  0.9657443 val bias auc =  0.9314014\n",
      "Validation Bias AUC score increased from  0.9301085 to 0.9314014 Saving the model at LSTM_GRU_fold_1_seed_42.pt\n",
      "\n",
      "\n",
      "Epoch 6/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  285 train loss =  0.1930398 train auc =  0.9699651 val loss =  0.1940637 val auc =  0.9659254 val bias auc =  0.9331131\n",
      "Validation Bias AUC score increased from  0.9314014 to 0.9331131 Saving the model at LSTM_GRU_fold_1_seed_42.pt\n",
      "\n",
      "\n",
      "Epoch 7/7:\n",
      "1443899/1443899\n",
      "360975/360975 Predictions done on validation data in 25 seconds\n",
      "97320/97320 Predictions done on test data in 7 seconds\n",
      "Seconds =  285 train loss =  0.1810305 train auc =  0.9734275 val loss =  0.1835463 val auc =  0.9666439 val bias auc =  0.9332135\n",
      "Validation Bias AUC score increased from  0.9331131 to 0.9332135 Saving the model at LSTM_GRU_fold_1_seed_42.pt\n",
      "\n",
      "\n",
      "The validation bias auc score after blend is 0.9332135348075389\n"
     ]
    }
   ],
   "source": [
    "n_folds=5\n",
    "kfold=StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=SEED)\n",
    "\n",
    "batches=512\n",
    "ep=[7]*n_folds\n",
    "\n",
    "auc_scores=np.zeros((n_folds,))\n",
    "bias_auc_scores=np.zeros((n_folds,))\n",
    "\n",
    "# oof\n",
    "oof_preds=np.zeros((X.shape[0],))\n",
    "# test preds\n",
    "test_preds=np.zeros((X_test.shape[0],))\n",
    "\n",
    "test_iterator = make_test_dataset(X_test,batch_size=512)\n",
    "m_test=X_test.shape[0]\n",
    "\n",
    "# enables averaging over epochs \n",
    "is_avg = False\n",
    "if is_avg:\n",
    "    epoch_list = [3,4,5] # meaning 3rd,4th and 5th epoch averaging\n",
    "else:\n",
    "    epoch_list = None   \n",
    "\n",
    "for fold,(train_index,val_index) in enumerate(kfold.split(X,train['target'].values)):\n",
    "    if fold == 0:\n",
    "        X_train,X_val=X[train_index].copy(),X[val_index].copy()\n",
    "        y_train,y_val=y[train_index].copy(),y[val_index].copy()\n",
    "\n",
    "        m_train,m_val=X_train.shape[0],X_val.shape[0]\n",
    "        input_dim=X_train.shape[1]\n",
    "\n",
    "        print(\"================================= FOLD\",fold+1,\"=============================================\")\n",
    "\n",
    "        print(\"Training Shape:\",X_train.shape,y_train.shape)\n",
    "        print(\"Validation Shape:\",X_val.shape,y_val.shape)\n",
    "\n",
    "        train_iterator=make_train_dataset(X_train,y_train,batch_size=batches,weights=weights[train_index].copy())\n",
    "        val_iterator=make_val_dataset(X_val,y_val,batch_size=512)\n",
    "\n",
    "        gc.enable()\n",
    "        del X_train,y_train,X_val\n",
    "        gc.collect()\n",
    "\n",
    "        model=initialize_model(embeddings_matrix,y_aux.shape[1])\n",
    "\n",
    "        base_lr=5e-4\n",
    "        is_scheduler=True\n",
    "        if is_scheduler:\n",
    "            max_lr=1e-2\n",
    "            # 3.5 times the iteration in an epoch\n",
    "            step_size=int(3.5*(m_train/batches + int(m_train%batches>0)))\n",
    "            optimizer=torch.optim.Adam(model.parameters(),lr=max_lr)\n",
    "            scheduler=CyclicLR(optimizer,base_lr=base_lr,max_lr=max_lr,\n",
    "                                  step_size=step_size,mode='triangular')\n",
    "        else:\n",
    "            optimizer=torch.optim.Adam(model.parameters(),lr=base_lr)\n",
    "            scheduler=None\n",
    "\n",
    "        loss_fn=BinaryFocalLoss(gamma=2)\n",
    "    #     loss_fn2=\n",
    "        epochs=ep[fold]\n",
    "        fname=\"LSTM_GRU_fold_\"+str(fold+1)+\"_seed_\"+str(SEED)+\".pt\"\n",
    "        evals=fit_data(model,optimizer,loss_fn,scheduler,train_iterator,val_iterator,\n",
    "                            m_train,m_val,epochs,fname,val_index,test_iterator,m_test)\n",
    "\n",
    "        # oof\n",
    "        if not is_avg:\n",
    "            oof_preds[val_index] = evals['val_preds'][evals['best_epoch']]\n",
    "            test_preds = test_preds + evals['test_preds'][evals['best_epoch']]/n_folds\n",
    "        else:\n",
    "            for index,i in enumerate(epoch_list):\n",
    "                oof_preds[val_index] = oof_preds[val_index] + evals['val_preds'][i]/len(epoch_list)\n",
    "                test_preds = test_preds + evals['test_preds'][i]/(len(epoch_list)*n_folds)\n",
    "\n",
    "        # appending the scores\n",
    "        auc_scores[fold]=roc_auc_score(y_val[:,0],oof_preds[val_index])\n",
    "        bias_auc_scores[fold]=compute_final_metric(val_index,oof_preds[val_index])[1]\n",
    "        print(\"The validation bias auc score after blend is\",bias_auc_scores[fold])\n",
    "\n",
    "        gc.enable()\n",
    "        del evals\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fold score are [0.96664387 0.         0.         0.         0.        ]\n",
      "The mean of the score is 0.1933288 and standard deviation between folds is 0.3866575\n"
     ]
    }
   ],
   "source": [
    "print(\"The Fold score are\",auc_scores)\n",
    "print(\"The mean of the score is\",np.round(np.mean(auc_scores),7),\"and standard deviation between folds is\",\\\n",
    "                 np.round(np.std(auc_scores),7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fold score are [0.93321353 0.         0.         0.         0.        ]\n",
      "The mean of the score is 0.1866427 and standard deviation between folds is 0.3732854\n"
     ]
    }
   ],
   "source": [
    "print(\"The Fold score are\",bias_auc_scores)\n",
    "print(\"The mean of the score is\",np.round(np.mean(bias_auc_scores),7),\"and standard deviation between folds is\",\\\n",
    "                 np.round(np.std(bias_auc_scores),7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias AUC on total train data is 0.5170592666983502\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>44484</td>\n",
       "      <td>0.516620</td>\n",
       "      <td>0.517907</td>\n",
       "      <td>0.517726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>53429</td>\n",
       "      <td>0.515355</td>\n",
       "      <td>0.517618</td>\n",
       "      <td>0.516884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>10997</td>\n",
       "      <td>0.514665</td>\n",
       "      <td>0.515223</td>\n",
       "      <td>0.519013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>40423</td>\n",
       "      <td>0.519213</td>\n",
       "      <td>0.518604</td>\n",
       "      <td>0.519420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>7651</td>\n",
       "      <td>0.503969</td>\n",
       "      <td>0.516787</td>\n",
       "      <td>0.506568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>21006</td>\n",
       "      <td>0.516065</td>\n",
       "      <td>0.516634</td>\n",
       "      <td>0.519266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>14901</td>\n",
       "      <td>0.513774</td>\n",
       "      <td>0.516947</td>\n",
       "      <td>0.516291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>25082</td>\n",
       "      <td>0.513269</td>\n",
       "      <td>0.514776</td>\n",
       "      <td>0.518131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>4889</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.517826</td>\n",
       "      <td>0.525584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size    ...     bpsn_auc  bnsp_auc\n",
       "0                           male          44484    ...     0.517907  0.517726\n",
       "1                         female          53429    ...     0.517618  0.516884\n",
       "2      homosexual_gay_or_lesbian          10997    ...     0.515223  0.519013\n",
       "3                      christian          40423    ...     0.518604  0.519420\n",
       "4                         jewish           7651    ...     0.516787  0.506568\n",
       "5                         muslim          21006    ...     0.516634  0.519266\n",
       "6                          black          14901    ...     0.516947  0.516291\n",
       "7                          white          25082    ...     0.514776  0.518131\n",
       "8  psychiatric_or_mental_illness           4889    ...     0.517826  0.525584\n",
       "\n",
       "[9 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 6.92 s, total: 20.6 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bias_df,score=compute_final_metric(train.index,oof_preds)\n",
    "print(\"Bias AUC on total train data is\",score)\n",
    "display(bias_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(oof_preds,hist=False,norm_hist=True)\n",
    "sns.distplot(test_preds,hist=False,norm_hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING THE SUBMISSION FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.024195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.839716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  prediction\n",
       "0  59848    0.000000\n",
       "1  59849    0.000000\n",
       "2  59852    0.024195\n",
       "3  59855    0.000000\n",
       "4  59856    0.839716"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof=pd.DataFrame()\n",
    "oof['id']=train['id']\n",
    "oof['prediction']=oof_preds\n",
    "oof.to_csv(\"oof.csv\",index=False)\n",
    "print(oof.shape)\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.010284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.004150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.027340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.012665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.165652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.010284\n",
       "1  7000001    0.004150\n",
       "2  7000002    0.027340\n",
       "3  7000003    0.012665\n",
       "4  7000004    0.165652"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['prediction']=test_preds\n",
    "sample.to_csv(\"submission.csv\",index=False)\n",
    "print(sample.shape)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total runtime of the kernel is 2804 seconds\n"
     ]
    }
   ],
   "source": [
    "t2=datetime.datetime.now()\n",
    "print(\"The total runtime of the kernel is\",round((t2-t1).total_seconds()),\"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
