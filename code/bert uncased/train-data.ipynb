{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import operator\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pkg_resources\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "# stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import shutil\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "tqdm.pandas()\n",
    "t1 = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "SEED = 42\n",
    "EPOCHS = 1\n",
    "Data_dir=\"../input/jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "Input_dir = \"../input\"\n",
    "WORK_DIR = \"../working/\"\n",
    "total_train_size = 1804874\n",
    "num_to_load = 1000000                         # Train size to match time limit\n",
    "valid_size = 97320                            # Validation Size\n",
    "TOXICITY_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Bart Pytorch repo to the PATH\n",
    "# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "package_dir_a = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "# bert base uncased....\n",
    "BERT_MODEL_PATH = \"../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/\"\n",
    "sys.path.insert(0, package_dir_a)\n",
    "\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45)\n",
      "CPU times: user 15.5 s, sys: 3.56 s, total: 19.1 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv(os.path.join(Data_dir,\"train.csv\"))\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are using cased bert model\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "\n",
    "# # Converting the lines to BERT format\n",
    "# # Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "# tokenizer and sequence length will be global variables. \n",
    "def convert_lines(x):\n",
    "    tokens_a = tokenizer.tokenize(x)\n",
    "    if len(tokens_a) > MAX_SEQUENCE_LENGTH:\n",
    "        tokens_a = tokens_a[:MAX_SEQUENCE_LENGTH]\n",
    "    \n",
    "    one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + \\\n",
    "                [0]*(MAX_SEQUENCE_LENGTH-len(tokens_a))\n",
    "    \n",
    "    return np.array(one_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all comment_text values are strings\n",
    "train['comment_text'] = train['comment_text'].astype(str) \n",
    "\n",
    "# takes about 35 minutes for the total train data. \n",
    "# sequences = np.array(train['comment_text'].progress_apply(convert_lines).tolist())\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 202)\n",
      "CPU times: user 43.2 s, sys: 17.9 s, total: 1min 1s\n",
      "Wall time: 16min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# takes 14 min\n",
    "sequences = np.array(Parallel(n_jobs=4,backend=\"multiprocessing\")(delayed(convert_lines)(x) for x in train['comment_text']))\n",
    "print(sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 202)\n"
     ]
    }
   ],
   "source": [
    "print(sequences.shape)\n",
    "file_name = \"first_lines_array_max_length_\"+str(MAX_SEQUENCE_LENGTH+2)\n",
    "np.save(file_name,sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
